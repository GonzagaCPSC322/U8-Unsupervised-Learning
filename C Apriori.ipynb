{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Apriori\n",
    "What are our learning objectives for this lesson?\n",
    "* Introduce the apriori algorithm\n",
    "    * Find supported itemsets\n",
    "    * Generate rules from supported itemsets\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Starter Code\n",
    "1. Open AssociationRuleFun/main.py and copy and paste the following Python list representing our Apriori Lab dataset:\n",
    "```python\n",
    "transactions = [\n",
    "    [\"b\", \"c\", \"m\"],\n",
    "    [\"b\", \"c\", \"e\", \"m\", \"s\"],\n",
    "    [\"b\"],\n",
    "    [\"c\", \"e\", \"s\"],\n",
    "    [\"c\"],\n",
    "    [\"b\", \"c\", \"s\"],\n",
    "    [\"c\", \"e\", \"s\"],\n",
    "    [\"c\", \"e\"]\n",
    "]\n",
    "```\n",
    "1. PA8 starter code functions:\n",
    "    1. Write a function, `compute_unique_values(table)`, that accepts a table and returns $I$, the set of all possible (unique) items in the table. For example, for `transactions`, $I$ = {b, c, e, m, s}. Test your function with the interview dataset and the fake MBA `transactions` dataset.\n",
    "    1. Write a function, `compute_k_minus_1_subsets(itemset)`, that returns all k-1-sized subsets of a list of size k. For example, for S = {c, e, s}, all k-1-sized subsets is {{c, e}, {c, s}, {e, s}}.\n",
    "        1. Note: try solving this without the `itertools` module I showed you :)\n",
    "    1. Stub out a function `apriori(table, minsup, minconf)` that accepts a table of transactions, a value for minimum rule support, and a value for minimum rule confidence. This function implements the Apriori algorithm and returns a list of rules.\n",
    "1. PA8 starter code function solutions are pushed to Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Market Basket\" Analysis\n",
    "In Market Basket Analysis\n",
    "* Dataset consists of \"transactions\" (one per \"row\")\n",
    "* Each transaction has a set of items (an \"itemset\")\n",
    "\n",
    "Grocery store example ...\n",
    "* T1 {bread, cheese, milk}\n",
    "* T2 {bread, cheese, fish, milk, sugar}\n",
    "* T3 {bread, cheese, milk}\n",
    "\n",
    "Note: T1 and T3 have same itemset\n",
    "\n",
    "* Often itemsets ordered to make calculations easier (example later)\n",
    "* Implication is items in itemset \"go\" together (e.g., purchased in a sale)\n",
    "* $I$ represent the set of all possible items in a dataset\n",
    "\n",
    "## Association Rules for Itemsets\n",
    "If left itemset THEN right itemset\n",
    "* E.g., IF {bread, sugar} THEN {cheese, milk}\n",
    "* Implies buying bread and sugar is associated with buying cheese and milk\n",
    "* Again, causality is not implied\n",
    "\n",
    "We'll write the above as $L \\rightarrow R$ (e.g., ${bread, sugar} \\rightarrow {cheese, milk}$)\n",
    "\n",
    "## Support for Itemsets\n",
    "For an itemset S\n",
    "* $support(S) = \\frac{count(S)}{N}$\n",
    "    * where $N$ = total # of transactions\n",
    "    * The $count(S)$ is the number of transactions $T$ where $S \\subseteq T$\n",
    "\n",
    "If $S = L \\cup R$ for $Rule_1$: IF $L$ THEN $R$\n",
    "* $support(S)$ (or similarly, $support(Rule_1)$) is same as before $N_{both}/N_{total}$\n",
    "* Just within subset or equal condition ($\\subseteq$)\n",
    "\n",
    "## Confidence for Itemsets\n",
    "If $S = L \\cup R$ for a rule $Rule_1$: IF $L$ THEN $R$\n",
    "* $confidence(Rule_1) = \\frac{count(L \\cup R)}{count(L)} = \\frac{count(S)}{count(L)}$\n",
    "    * Same as before: $N_{both}/N_{left}$\n",
    "\n",
    "Often, only interested in \"confident\" and \"supported\" rules, i.e., where the confidence is at least $minconf$ (e.g., 80%) and support is at least $minsup$ (e.g., 1%)\n",
    "\n",
    "### Lab Task 1\n",
    "Given the following example market basket analysis (Fake) data:\n",
    "\n",
    "|transaction |itemset|\n",
    "|-|-|\n",
    "|1 |{b, c, m}|\n",
    "|2 |{b, c, e, m, s}|\n",
    "|3 |{b}|\n",
    "|4 |{c, e, s}|\n",
    "|5 |{c}|\n",
    "|6 |{b, c, s}|\n",
    "|7 |{c, e, s}|\n",
    "|8 |{c, e}|\n",
    "\n",
    "Where b = bread, c = cheese, e = eggs, m = milk, s = sugar and each transaction is in alphabetical order.\n",
    "\n",
    "1. Find I, the set of all possible items in the dataset.\n",
    "1. Let itemset S = {c, e}. \n",
    "    1. A transaction T \"matches\" S if $S \\subseteq T$. Find all the matches of S.\n",
    "    1. What percentage of the dataset matches S? This is the support of S.\n",
    "\n",
    "### Lab Task 2\n",
    "Calculate count(L), count(R), count($L \\cup R$), confidence, and support for the following rules:\n",
    "1. IF {b,c} THEN {m}\n",
    "1. IF {e} THEN {s}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Apriori Algorithm\n",
    "Leverages the fact that:\n",
    "* If an itemset is supported, all of its (non-empty) subsets are also supported\n",
    "* Removing an item cannot reduce # of matching transactions\n",
    "* e.g., {c,m} $\\rightarrow$ {b} is supported, and so {c} $\\rightarrow$ {b} is also supported\n",
    "\n",
    "The fact implies the following\n",
    "* Let $L_k$ be the set of supported itemsets with $k$ items\n",
    "* If $L_k$ is empty (not supported), then $L_{k+1}, L_{k+2}, ...$ will be empty too (i.e., these sets will not be supported either either)\n",
    "* ... cant improve support by adding items\n",
    "\n",
    "Basic Apriori Algorithm (\"bottom up\")\n",
    "1. Find $L_1$ (the set of 1-item supported itemsets)\n",
    "2. Generate $L_2$ from $L_1$\n",
    "3. Continue ($L_3$ from $L_2$, $L_4$ from $L_3$, etc.) until $L_k = \\emptyset$\n",
    "4. Generate rules from resulting itemsets\n",
    "\n",
    "Generating $L_k$ from $L_{k−1}$ (Step 2)\n",
    "* Create a \"candidate\" itemset $C_k$ from $L_{k−1}$ ... assumes sorted itemsets\n",
    "    * (i) For each $A \\in L_{k−1}$ and $B \\in L_{k−1} (A \\neq B)$\n",
    "        * (ii)     If $A[0:-1] == B[0:-1]$\n",
    "            * (iii)        Add $A \\cup B$ to $C_k$ **unless**\n",
    "                * (iv)             a $k - 1$ subset of $A \\cup B \\notin L_{k−1}$\n",
    "* Set $L_k$ to supported itemsets in $C_k$\n",
    "\n",
    "Step (ii) implies all but last item in $A$ and $B$ match (for $A$ and $B$ sorted)\n",
    "\n",
    "Step (iv) prunes search space\n",
    "* If a $k - 1$ subset is not in $L_{k-1}$ then $A \\cup B$ must not be supported\n",
    "* This is the essential fact exploited by Apriori\n",
    "* That is, adding an item won't increase support\n",
    "\n",
    "Supported itemsets are $L_{2} \\cup ... \\cup L_k$ where $k$ is the last non-empty set\n",
    "* These are also represented solely by $L_k$ considering each elements subsets\n",
    "\n",
    "### Lab Task 3\n",
    "Using Apriori, find all supported itemsets assuming $minsup$ = 25%\n",
    "\n",
    "<!-- \n",
    "\n",
    "* $L_1$ = {b}, {c}, {e}, {m}, {s}\n",
    "* $C_2$ = {b, c}, {b, e}, {b, m}, {b, s}, {c, e}, {c, m}, {c, s}, {e, m}, {e, s}, {m, s}\n",
    "* $L_2$ = {b, c}, {b, m}, {b, s}, {c, e}, {c, m}, {c, s}, {e, s}\n",
    "* $C_3$ = {b, c, m}, {b, c, s}, {b, m, s}, {c, e, m}, {c, e, s}, {c, m, s}\n",
    "* $L_3$ = {b, c, m}, {b, c, s}, {c, e, s}\n",
    "* $C_4$ = $\\emptyset$\n",
    "* $L_4$ = $\\emptyset$ \n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Rules from Itemsets\n",
    "Naive (brute force) algorithm\n",
    "\n",
    "Approach 1: Using supported itemsets are $L_{2} \\cup ... \\cup L_k$ where $k$ is the last non-empty set\n",
    "* Given itemset $S$, find all RHSs\n",
    "    * Start with 1-item RHS, move to 2-item RHS, etc.\n",
    "    * For each rule, remaining items not in RHS become LHS\n",
    "    \n",
    "Approach 2: Using solely $L_k$ considering each elements subsets\n",
    "* Given itemset $S$, find all RHSs\n",
    "    * Start with 1-item RHS, move to 2-item RHS, etc.\n",
    "    * For each rule, start with 1-item LHS, move to 2-item LHS, etc. using remaining items not in RHS\n",
    "\n",
    "### Lab Task 4\n",
    "Find all rules for S = {b,c,m} and compute confidence. Keep rules assuming $minconf$ = 80%.\n",
    "\n",
    "<!-- \n",
    "\n",
    "|LHS |RHS |Confidence ($N_{both}/N_{left}$)|\n",
    "|-|-|-|\n",
    "|{c, m}$ \\rightarrow$  |{b} |**100% (2/2)**|\n",
    "|{b, m}$ \\rightarrow$ |{c} |**100% (2/2)**|\n",
    "|{b, c}$ \\rightarrow$ |{m} |66% (2/3)|\n",
    "|{m}$ \\rightarrow$ |{b, c} |**100% (2/2)**|\n",
    "|{c}$ \\rightarrow$ |{b, m} |29% (2/7)|\n",
    "|{b}$ \\rightarrow$ |{c, m} |50% (2/4)| \n",
    "\n",
    "-->\n",
    "\n",
    "\n",
    "### Lab Task 5 (For Extra Practice)\n",
    "Repeat the previous task for the remaining itemsets S in $L_{2} \\cup ... \\cup L_k$ where $k$ is the last non-empty set (e.g. {b, c, m}, {b, c, s}, {c, e, s}, {b, c}, {b, m}, {b, s}, {c, e}, {c, m}, {c, s}, {e, s}). The set of all rules left is your final apriori set of association rules :)\n",
    "\n",
    "## Thoughts on Apriori Improvements/Optimizations\n",
    "The problem with this approach:\n",
    "* For an itemset with $k$ items\n",
    "* There are $2^{k}$ - 2 RHSs! (again, all subsets, e.g. power set)\n",
    "    * \"-2\" since we don't count set of all items and empty set (for RHS)\n",
    "* Thus, exponential in the size of itemsets\n",
    "\n",
    "Instead use the following fact:\n",
    "* Moving items from LHS to RHS cannot increase rule confidence\n",
    "    * If $R1 = A \\cup B \\rightarrow C$\n",
    "    * We have $conf(R1) = \\frac{count(A \\cup B \\cup C)}{count(A \\cup B)}$\n",
    "    * If $R2 = A \\rightarrow B \\cup C$ ... move $B$ to RHS\n",
    "    * We have $conf(R2) = \\frac{count(A \\cup B \\cup C)}{count(A)}$\n",
    "    * And always true that $count(A) \\geq count(A \\cup B)$\n",
    "        * Again, adding more items cannot increase # of matching transactions\n",
    "\n",
    "From the example:\n",
    "* {b, c} $\\rightarrow$ {m}... $conf$ = 66% (2/3)\n",
    "* {c} $\\rightarrow$ {b, m}... $conf$ = 29% (2/7)\n",
    "* {b} $\\rightarrow$ {c, m}... $conf$ = 50% (2/4)\n",
    "\n",
    "* Given the first rule, we knew second two would be $\\leq$ 66%\n",
    "* Which is below, e.g., $minconf$ = 80%\n",
    "\n",
    "This means we can \"short-circuit\" rule generation\n",
    "* If no rules with a RHS of size $k$ are \"confident\"... stop searching for rules\n",
    "    * Since algorithm proceeds $k = 1, k = 2, ...$\n",
    "    * Confident means confidence $\\geq minconf$\n",
    "* Can also record RHSs not to consider\n",
    "    * e.g., any that are a superset of $\\{m\\}$\n",
    "    \n",
    "Q: Can we apply Apriori to tabular data?\n",
    "* YES! (assuming categorical or discretized values)\n",
    "* Each row becomes a transaction (itemset) of attribute-value pair items\n",
    "* Itemsets restricted though, e.g., $\\{s = 1, j = 3\\}$ but not $\\{s = 1, s = 2\\}$ for attributes $s$ and $j$.\n",
    "* This affects how itemsets are combined in Apriori\n",
    "\n",
    "    \n",
    "## Apriori Rule Interestingness Measures\n",
    "* There can still be many rules satisfying $minconf$ and $minsup$\n",
    "* Various metrics for further determining \"interestingness\"\n",
    "\n",
    "### Leverage\n",
    "* The difference between support of $A \\cup B$ and support of $A$ and $B$ if independent\n",
    "$$leverage(A \\rightarrow B) = support(A \\cup B) - support(A) \\times support(B)$$\n",
    "* Interested in \"improvement\" of support\n",
    "    * e.g., if $support(A)$ = 10% and $support(B)$ = 10%, if independent we'd expect them to occur together approximately 1% of the time\n",
    "* Typically set $minleverage$ low, e.g., 0.0001 (improve 1 in every 10,000 transactions)\n",
    "\n",
    "### Lift\n",
    "* Similar to leverage\n",
    "* But instead of difference, measures how many more times $A$ and $B$ occur together (than if independent)\n",
    "$$lift(A \\rightarrow B) = \\frac{support(A \\cup B)}{support(A) \\times support(B)}$$\n",
    "* Lift values greater than 1 are considered \"interesting\"\n",
    "* But with all metrics, you must use common sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 6 (For Extra Practice)\n",
    "Using the transactions dataset, compute leverage and lift for the rule {c, m} $\\rightarrow$ {b} \n",
    "1. $leverage(A \\rightarrow B) = support(A \\cup B) - support(A) \\times support(B)$\n",
    "    1. Want leverage > minleverage (e.g. 0.0001)\n",
    "1. $lift(A \\rightarrow B) = \\frac{support(A \\cup B)}{support(A) \\times support(B)}$\n",
    "    1. Want lift > 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
