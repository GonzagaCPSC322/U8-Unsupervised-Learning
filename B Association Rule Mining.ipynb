{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Association Rule Mining\n",
    "What are our learning objectives for this lesson?\n",
    "* Introduce association rule mining\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 12/3\n",
    "* Announcements\n",
    "    * Mid-project demo is due on or before today\n",
    "            * EDA (at least show chart of class distribution) and preliminary classification results\n",
    "    * Next class\n",
    "        * Course evals\n",
    "        * Project ideas to keep learning beyond this class\n",
    "        * IQ10 on U7 and U8 topics\n",
    "        * Going over how the project presentations will work\n",
    "            * Note: if your laptop requires an adapter to connect to HDMI, please bring it next class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s) \n",
    "1. Please go to Zagweb and fill out the course evaluation for this class, thank you!!\n",
    "1. Go to the [Project Presentation Schedule](https://docs.google.com/spreadsheets/d/1sQ1GW5ldQFkJWO_bUXJtH0cwxCfbdMULciK1JZv9lfA/edit#gid=0) on Google Drive and paste your team's poster presentation link\n",
    "1. Create a new folder called AssociationRuleFun. Copy the interview dataset (available below) into a main.py\n",
    "    * In AssociationRuleFun/main.py, define a function that prepends the class label and \"=\" before each attribute value in the table. For example, the table after your function updates it should have values that look like this:\n",
    "```\n",
    "level=Senior  lang=Java    tweets=no   phd=no   interviewed_well=False\n",
    "level=Senior  lang=Java    tweets=no   phd=yes  interviewed_well=False\n",
    "level=Mid     lang=Python  tweets=no   phd=no   interviewed_well=True\n",
    "level=Junior  lang=Python  tweets=no   phd=no   interviewed_well=True\n",
    "level=Junior  lang=R       tweets=yes  phd=no   interviewed_well=True\n",
    "level=Junior  lang=R       tweets=yes  phd=yes  interviewed_well=False\n",
    "level=Mid     lang=R       tweets=yes  phd=yes  interviewed_well=True\n",
    "level=Senior  lang=Python  tweets=no   phd=no   interviewed_well=False\n",
    "level=Senior  lang=R       tweets=yes  phd=no   interviewed_well=True\n",
    "level=Junior  lang=Python  tweets=yes  phd=no   interviewed_well=True\n",
    "level=Senior  lang=Python  tweets=yes  phd=yes  interviewed_well=True\n",
    "level=Mid     lang=Python  tweets=no   phd=yes  interviewed_well=True\n",
    "level=Mid     lang=Java    tweets=yes  phd=no   interviewed_well=True\n",
    "level=Junior  lang=Python  tweets=no   phd=yes  interviewed_well=False\n",
    "```\n",
    "\n",
    "Interview dataset:\n",
    "```\n",
    "header = [\"level\", \"lang\", \"tweets\", \"phd\", \"interviewed_well\"]\n",
    "table = [\n",
    "        [\"Senior\", \"Java\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"Java\", \"no\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"R\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Java\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"yes\", \"False\"]\n",
    "    ]\n",
    "```\n",
    "\n",
    "Then, work on\n",
    "    1. `compute_rule_counts(rule, table)`\n",
    "    1. `compute_rule_interestingness(rule, table)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * Project presentations are on Wednesday 12/11 at 10:30am (meet in Bollier 003)\n",
    "        * Let's go over the plan\n",
    "* Finish Intro to ARM lab\n",
    "* Ideas for continuing learning\n",
    "    * Association rule mining: Read Bramer ch. 16 & 17 and implement Apriori algorithm\n",
    "    * ML in the cloud: ast set of notes in U8 on Github is on [Amazon SageMaker](https://aws.amazon.com/sagemaker/) if you are interested: \"Build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows\"\n",
    "    * Neural nets: I posted a PDF of a book, Build Your Own Neural Network, on Google Drive. It uses Python and builds up a NN from scratch. I highly recommend it!\n",
    "* IQ10 last ~15 mins of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule Mining\n",
    "* Tries to find any rule, not just those that predict a class label\n",
    "* Any attribute(s) becomes the label\n",
    "* Example of \"unsupervised learning\"\n",
    "\n",
    "Sometimes expressed as IF-THEN rules with conjunctive \"bodies\" and \"heads\"\n",
    "* IF $att_i = val_i \\wedge att_{k} = val_k \\wedge$ ... THEN $att_u = val_u \\wedge att_z = val_z \\wedge$ ...\n",
    "    * The left-hand side (LHS) is the rule body\n",
    "    * The right-hand side (RHS) is the rule head\n",
    "\n",
    "Similar to decision trees, but...\n",
    "* Head attributes don't have to be designated labels\n",
    "* Can have multiple attributes in the head\n",
    "\n",
    "Rules suggest a possible association\n",
    "* Between values of LHS attributes and values of RHS attributes\n",
    "* Rules do not suggest causality!\n",
    "\n",
    "Q: Brainstorm ways to perform rule mining ...\n",
    "* Brute force (i.e., try all combinations)\n",
    "* There are tricks to do better (we'll discuss some of these)\n",
    "\n",
    "Q: What are the issues/difficulties of rule mining?\n",
    "* Performance!\n",
    "    * Lots of combinations\n",
    "    * Even when using the performance \"tricks\"\n",
    "* Low \"signal\"... the \"interestingness\" of rules\n",
    "    * Rules may be very accurate but also rare\n",
    "    * e.g., body rarely occurs and/or head rarely occurs... think overfitting\n",
    "    * Recall independence assumption in Naive Bayes\n",
    "\n",
    "Rules likely **will not** be 100% accurate\n",
    "* Need metrics for measuring rule accuracy\n",
    "* As well as \"interestingness\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interestingness Measures\n",
    "Basics:\n",
    "* IF left THEN right\n",
    "    * $N_{left}$ = # of instances matching left\n",
    "    * $N_{right}$ = # of instances matching right\n",
    "    * $N_{both}$ = # of instances matching both left and right\n",
    "    * $N_{total}$ = # of instances in the dataset\n",
    "\n",
    "Confidence\n",
    "* Proportion of RHSs predicted by the rule that are correctly predicted\n",
    "$$\\frac{N_{both}}{N_{left}}$$\n",
    "* Similar in spirit to accuracy ($\\frac{TP+TN}{P+N}$)\n",
    "\n",
    "Support\n",
    "* Proportion of data set correctly predicted by the rule\n",
    "$$\\frac{N_{both}}{N_{total}}$$\n",
    "* One measure of \"interestingness\" (or \"rareness\")\n",
    "\n",
    "Completeness\n",
    "* Proportion of matching RHSs correctly predicted by the rule\n",
    "$$\\frac{N_{both}}{N_{right}}$$\n",
    "* Similar to estimate for $P(left|right)$\n",
    "\n",
    "We'll look at more later ...\n",
    "\n",
    "### Lab Task 1\n",
    "For the interview dataset, calculate $N_{left}$, $N_{right}$, $N_{both}$, $N_{total}$, confidence, support, and completeness for the following rules:\n",
    "1. IF interviewed_well=False THEN tweets=no  \n",
    "1. IF level=Mid THEN interviewed_well=True \n",
    "1. IF tweets=yes THEN interviewed_well=True\n",
    "1. IF lang=R THEN tweets=yes\n",
    "1. IF phd=no AND tweets=yes THEN interviewed_well=True\n",
    "\n",
    "|level|lang|tweets|phd|interviewed_well|\n",
    "|-|-|-|-|-|\n",
    "|Senior|Java|no|no|False|\n",
    "|Senior|Java|no|yes|False|\n",
    "|Mid|Python|no|no|True|\n",
    "|Junior|Python|no|no|True|\n",
    "|Junior|R|yes|no|True|\n",
    "|Junior|R|yes|yes|False|\n",
    "|Mid|R|yes|yes|True|\n",
    "|Senior|Python|no|no|False|\n",
    "|Senior|R|yes|no|True|\n",
    "|Junior|Python|yes|no|True|\n",
    "|Senior|Python|yes|yes|True|\n",
    "|Mid|Python|no|yes|True|\n",
    "|Mid|Java|yes|no|True|\n",
    "|Junior|Python|no|yes|False|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 2\n",
    "In AssociationRuleFun/main.py, define a function `compute_rule_counts(rule, table)` that computes $N_{left}$, $N_{right}$, $N_{both}$, $N_{total}$ for the rule using the interview dataset.\n",
    "\n",
    "### Lab Task 3\n",
    "In AssociationRuleFun/main.py, define a function `compute_rule_interestingness(rule, table)` that computes confidence, support, and completeness for the rule using the interview dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 4 (For Extra Practice)\n",
    "For the fake iPhone Purchases dataset, calculate $N_{left}$, $N_{right}$, $N_{both}$, $N_{total}$, confidence, support, and completeness for the following two example rules:\n",
    " 1. IF buys_iphone==yes AND standing==2 THEN credit_rating==fair\n",
    " 1. IF credit_rating==fair THEN standing==2 AND job_status==2\n",
    " \n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "### Lab Task 5 (For Extra Practice)\n",
    "Assume we only know the domains of each attribute in the iPhone dataset. What is the maximum number of RHSs for the dataset? (Hint: at least one attribute is always used in the LHS of a rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## J-Measure\n",
    "Basic Idea\n",
    "* Find the best $N$ rules from $D$ ... $N$ is a parameter\n",
    "* Use $J$-Measure as a quality metric for rules\n",
    "* Prune search space for rules (search \"strategy\")\n",
    "\n",
    "Parameters\n",
    "* Number of rules $N$ (often small ... 10 to 20)\n",
    "* Number of LHS terms to consider (max number) ... \"rule order\"\n",
    "* Number of RHS terms to consider (e.g., 1)\n",
    "\n",
    "The Basic Search Strategy Assume RHSs have 1 term\n",
    "1. Generate all RHSs\n",
    "2. Generate all order 1 rules\n",
    "3. Calculate $J$-value for each rule\n",
    "4. Keep the top $N$ rules\n",
    "5. \"Specialize\" best $N$ rules by increasing order, again keeping only the top $N$ rules (i.e., from the original $N$ + specialized)\n",
    "\n",
    "Referred to as a \"beam\" search\n",
    "* Beam width is $N$\n",
    "* Not guaranteed to find best rules\n",
    "* Can still be expensive\n",
    "\n",
    "To reduce the search space further, can exploit $J_{max}$\n",
    "* Can test if specializing a rule can improve $J$-Value\n",
    "* If not, skip it\n",
    "\n",
    "## Measuring the Information Content of Rules\n",
    "* Given a rule:\n",
    "\n",
    "Rule1: IF left THEN right  \n",
    "... or just ...  \n",
    "Rule1: IF L THEN R\n",
    "\n",
    "* The $J$-Value is: \n",
    "$$J(Rule1) = P(L) \\times j(Rule1)$$\n",
    "* Where $j$ is the \"cross entropy\" (measured in bits of information)\n",
    "$$j(Rule1) = P(R|L) \\times log_{2}\\frac{P(R|L)}{P(R)} + (1 - P(R|L)) \\times log_{2}\\frac{1 - P(R|L)}{1 - P(R)}$$\n",
    "    * Roughly the amount of information (average number of bits) needed to distinguish between $P(R|L)$ and $1 - P(R|L)$\n",
    "    * The more information needed, the more dissimilar the two are\n",
    "* The P's are estimated as:\n",
    "$$P(R) = \\frac{N_{right}}{N_{total}}$$\n",
    "$$P(L) = \\frac{N_{left}}{N_{total}}$$\n",
    "$$P(R|L) = \\frac{N_{both}}{N_{left}}$$ ... confidence!\n",
    "\n",
    "## \"Specializing\" Rules\n",
    "* Adding terms to the LHS (increasing the order)\n",
    "* $J$-Value of a rule obtained through specializing a given rule is bounded by:\n",
    "$$J_{max} = P(L) \\times max\\{P(R|L) \\times log_{2}\\frac{1}{P(R)}, (1 - P(R|L)) \\times log_{2}\\frac{1}{1 - P(R)}\\}$$\n",
    "\n",
    "* So, e.g., if a rule $Rule_i$ already has $J(Rule_i) = J_{max}(Rule_i)$, then no reason to further specialize (pruning)\n",
    "* Or, if $J_{max}(Rule_i)$ puts it out of the new top $N$, no reason to specialize further (also pruning)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
