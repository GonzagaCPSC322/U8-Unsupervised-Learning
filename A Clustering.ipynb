{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Clustering\n",
    "What are our learning objectives for this lesson?\n",
    "* Introduce k-means clustering\n",
    "* Evaluate the quality of a cluster using an objective function\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "1. Create a new folder called ClusteringFun. In ClusteringFun, create a main.py and paste the following T-shirt sizes data:\n",
    "```python\n",
    "header = [\"height(cm)\", \"weight(kg)\"] #, \"size(t-shirt)\"]\n",
    "X_train = [\n",
    "    [158, 58], # \"M\"\n",
    "    [158, 59], # \"M\"\n",
    "    [158, 63], # \"M\"\n",
    "    [160, 59], # \"M\"\n",
    "    [160, 60], # \"M\"\n",
    "    [163, 60], # \"M\"\n",
    "    [163, 61], # \"M\"\n",
    "    [160, 64], # \"L\"\n",
    "    [163, 64], # \"L\"\n",
    "    [165, 61], # \"L\"\n",
    "    [165, 62], # \"L\"\n",
    "    [165, 65], # \"L\"\n",
    "    [168, 62], # \"L\"\n",
    "    [168, 63], # \"L\"\n",
    "    [168, 66], # \"L\"\n",
    "    [170, 63], # \"L\"\n",
    "    [170, 64], # \"L\"\n",
    "    [170, 68] # \"L\"\n",
    "]\n",
    "# TODO: normalize data before calculating distances\n",
    "```\n",
    "1. Start on Clustering Lab task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * Nice job getting PA7 done!\n",
    "    * BONUS PA8 is posted and due 12/6 (no late work accepted). Please read through it before next class and come with questions\n",
    "    * I read everyone's project proposals and have some feedback on Canvas (can you see it?)\n",
    "    * Mid-project demo is due on or before 12/3 (bonus points for demoing earlier than 12/3 in office hours)\n",
    "        * EDA (at least show chart of class distribution) and preliminary classification results\n",
    "    * MA10 is due next class\n",
    "* Clustering lab and ClusteringFun\n",
    "* IQ9 last ~15 mins of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Given a collection of \"objects\" (i.e., instances with attributes), determine similar groups of objects (\"clusters\")\n",
    "\n",
    "### Lab Task 1\n",
    "What are possible clusters for the following objects with two attributes?\n",
    "* When k = 4 clusters?\n",
    "* When k = 5 clusters?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U7-Unsupervised-Learning/master/figures/cluster_example1.png\" width=\"450\"/>\n",
    "\n",
    "One possible set of solutions:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U7-Unsupervised-Learning/master/figures/cluster_example2.png\" width=\"450\"/>\n",
    "\n",
    "Like with $k$-nn, need a distance metric\n",
    "* To determine how close instances so we can form clusters\n",
    "* We'll use Euclidean distance\n",
    "\n",
    "## Centroids\n",
    "A centroid is the point in the center of a cluster. Using Euclidean distances, a cluster's centroid is its \"average point\" ...\n",
    "* Specifically: each attribute value of the centroid is the average of the corresponding attribute value of the points in the cluster\n",
    "\n",
    "### Lab Task 2\n",
    "On paper, plot the following points of a cluster and plot its centroid (center point AKA average point of a cluster).\n",
    "\n",
    "|att1 |att2|\n",
    "|-|-|\n",
    "|3 |4|\n",
    "|6 |2|\n",
    "|2 |1|\n",
    "|5 |5|\n",
    "\n",
    "## Cluster Quality\n",
    "The quality of the cluster is given by an \"objective function\"\n",
    "* i.e., a function we want to minimize (in this case)\n",
    "\n",
    "We'll use the \"Total Sum of Squares\" (TSS)\n",
    "* The sum of squared distances to the centroid from cluster instances\n",
    "$$TSS = \\sum_{i=1}^{n}((x_i - \\overline{x})^{2} + (y_i - \\overline{y})^{2})$$\n",
    "\n",
    "### Lab Task 3\n",
    "Calculate the TSS for the previous example.\n",
    "\n",
    "<!-- $$TSS = ((3 - 4)^2 + (4 - 3)^2) + ((6 - 4)^2 + (2 - 3)^2) +((2 - 4)^2 + (1 - 3)^2) + ((5 - 4)^2 + (5 - 3)^2) = 20$$\n",
    " -->\n",
    "\n",
    "Notes on the TSS:\n",
    "* Can work well here since we use Euclidean distance\n",
    "* Especially if we don't apply the square root function when calculating distances (can just add up the distances used)\n",
    "* TSS also penalizes bigger distances more\n",
    "\n",
    "## $k$-Means clustering algorithm\n",
    "1. Pick a value of $k$\n",
    "1. Select $k$ objects (arbitrarily) to use as initial centroids\n",
    "1. Assign each instance to the cluster of its nearest centroid\n",
    "1. Recalculate the centroids for the $k$ clusters\n",
    "1. Repeat Steps 3-4 until the centroids no longer move (change)\n",
    "\n",
    "Note that the resulting clusters depend on initial instances used as centroids\n",
    "* e.g., starting with different instances can change the outcome\n",
    "* One approach is to randomly pick $k$ instances as centroids\n",
    "\n",
    "Q: What happens when $k$ = 1?\n",
    "* We end up with only one cluster!\n",
    "\n",
    "Q: What happens when $k = n$ for $n$ the number of instances?\n",
    "* We end up with 1 cluster per instance (so, the original dataset)!\n",
    "\n",
    "Can find good values for k experimentally ...\n",
    "* In general, we want small values for $k$ (i.e., fewer clusters)\n",
    "* Start with $k$ = 2, then use TSS to measure quality\n",
    "* Move to $k$ = 3, $k$ = 4, and so on, until TSS begins to converge\n",
    "* Select smallest $k$ close to the convergence\n",
    "* See textbook for example\n",
    "\n",
    "Why clustering?\n",
    "* For prediction (e.g., determine instance's cluster, using voting)\n",
    "* For data reduction (reduce dataset to one instance per cluster)\n",
    "* For basic similarity search (e.g., find similar movies)\n",
    "* For data exploration\n",
    "\n",
    "### Lab Task 4\n",
    "Let's implement k-means clustering with k = 2 using height and weight data from [this site](https://www.listendata.com/2017/12/k-nearest-neighbor-step-by-step-tutorial.html)\n",
    "1. Starter code functions:\n",
    "    * `perform_k_means_clustering(table, k)`\n",
    "    * `compute_cluster_centroids(table, k)`\n",
    "    * `find_nearest_cluster(instance, centroids)`\n",
    "    * `check_clusters_moved(old_centroids, new_centroids)`\n",
    "1. (when your k-means algorithm is implemented) Is there a relationship between the clusters formed and T-shirt size?\n",
    "1. (when your k-means algorithm is implemented) Let's say there is a new customer named 'Monica' has height 161cm and weight 61kg. What cluster does Monica belong to? Can we conclude anything about her T shirt size?\n",
    "\n",
    "Note: because we use the Euclidean distance function, we don't want to forget to normalize the dataset before applying k-means clustering!!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
